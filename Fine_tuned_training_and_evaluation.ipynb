{"cells":[{"cell_type":"markdown","metadata":{"id":"y37_0N8OeMk7"},"source":["## Entraînement et évaluation"]},{"cell_type":"markdown","metadata":{"id":"xniyL7Ro6F4z"},"source":["Nous allons passer maintenant à l'entrainement et l'évaluation du modèle. Nous utiliserons principalement la bibliothèque trainer de *tranformers*. Pour cela nous nécessiterons de :\n","- Définir un **collateur de données**. \n","- une **métrique d'évaluation**\n","- partir d'un **modèle pré-entrainé**\n","- définir les **paramètres d'entrainement**\n"]},{"cell_type":"markdown","metadata":{"id":"bh_2H02Hn5BW"},"source":["## Importation"]},{"cell_type":"markdown","metadata":{"id":"iu-TUd9poHuR"},"source":["Nous commençons par installer les librairies nécéssaires. Ici nous installons huggigface afin d'enregistrer et exporter notre modèle ré-entraîné sur hugging face."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1653927344354,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"},"user_tz":-120},"id":"-VZbyvG4M8t5","outputId":"7c89438a-204a-4755-aca8-35e9b6ee072f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!pip install huggingface_hub\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["\"\"\"!pip install huggingface_hub\n","from huggingface_hub import notebook_login\n","\n","notebook_login()\"\"\""]},{"cell_type":"markdown","metadata":{"id":"01Wnfrsd9NW_"},"source":["On installe Git-LFS pour télécharger les points de contrôle de notre modèle :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3880,"status":"ok","timestamp":1653927348216,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"},"user_tz":-120},"id":"o2ci-MTY9UoT","outputId":"af9ad09e-f41a-4d42-e99e-6f07e551a6a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 33.1 MB/s \n","\u001b[?25hInstalling collected packages: pyyaml\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0\n"]}],"source":["!pip install --upgrade pyyaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTxMysP5vRoe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653927350564,"user_tz":-120,"elapsed":2357,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"}},"outputId":"ab9a5e20-2fde-44bc-cc5f-22176abd07ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","git-lfs is already the newest version (2.3.4-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"]}],"source":["!sudo apt-get install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"-iCWDBJKojD2"},"source":["Nous installons ensuite différents outils :\n","- *datasets* pour traiter les données en entrées\n","- *transformers* une brique du CTC\n","- *soundfile* pour traiter les audios\n","- *jiwer* pour calculer le taux d'erreur."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9btk3Ms_hRX"},"outputs":[],"source":["%%capture\n","!pip install git+https://github.com/huggingface/datasets.git\n","!pip install transformers==4.11.3\n","!pip install soundfile\n","!pip install jiwer"]},{"cell_type":"markdown","metadata":{"id":"-a8OqWI3MkH6"},"source":["Tout d'abord nous avons besoin d'importer les extracteurs de descripteurs, tokenizers ainsi que la base de données."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8A0SmhIy-fy"},"outputs":[],"source":["input_data_folder = \"/content/data/data_40/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ro7GtixJMyRm","executionInfo":{"status":"ok","timestamp":1653927429529,"user_tz":-120,"elapsed":35713,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"}},"outputId":"7d60c318-e69b-4305-b0f8-8f32460e3c6b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['speech', 'sampling_rate', 'target_text'],\n","        num_rows: 936\n","    })\n","    test: Dataset({\n","        features: ['speech', 'sampling_rate', 'target_text'],\n","        num_rows: 40\n","    })\n","})"]},"metadata":{},"execution_count":8}],"source":["from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Processor\n","processor = Wav2Vec2Processor.from_pretrained(input_data_folder+\"Processor\")  # Le processeur est une classe contenantle feature extractor ainsi que le tokenizer\n","\n","from datasets import load_from_disk\n","timit = load_from_disk(input_data_folder +\"dataset\")\n","timit_prepared = load_from_disk(input_data_folder +\"dataset_prepared\")\n","\n","timit"]},{"cell_type":"markdown","metadata":{"id":"udoh-S9VLNlK"},"source":["### Collateur de données"]},{"cell_type":"markdown","metadata":{"id":"3AwymEczKp0c"},"source":["Commençons par créer le collateur de données. Sans entrer dans les détails, à la différence des collateurs de données courants, ce collateur de données traite les valeurs d'entrée et les étiquettes différemment et leur applique donc des fonctions de remplissage distinctes (en utilisant à nouveau le gestionnaire de contexte de Wav2Vec2). Ceci est nécessaire car dans la parole, l'entrée et la sortie sont de modalités différentes, ce qui signifie qu'elles ne devraient pas être traitées par la même fonction de remplissage. De manière analogue aux collateurs de données communs, les padden tokens (tokens de remplissage) dans les étiquettes avec -100 de sorte que ces tokens ne sont pas pris en compte lors du calcul de la perte."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRqoBai4ekKg"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass, field\n","from typing import Any, Dict, List, Optional, Union\n","from transformers import Wav2Vec2Processor\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received.\n","    Args:\n","        processor (:class:`~transformers.Wav2Vec2Processor`)\n","            The processor used for proccessing the data.\n","        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence if provided).\n","            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","              maximum acceptable input length for the model if that argument is not provided.\n","            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","              different lengths).\n","        max_length (:obj:`int`, `optional`):\n","            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n","        max_length_labels (:obj:`int`, `optional`):\n","            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n","        pad_to_multiple_of (:obj:`int`, `optional`):\n","            If set will pad the sequence to a multiple of the provided value.\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","    max_length: Optional[int] = None\n","    max_length_labels: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    pad_to_multiple_of_labels: Optional[int] = None\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lenghts and need\n","        # different padding methods\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\"\n","        )\n","        with self.processor.as_target_processor():\n","            labels_batch = self.processor.pad(\n","                label_features,\n","                padding=self.padding,\n","                max_length=self.max_length_labels,\n","                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n","                return_tensors=\"pt\"\n","            )\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        batch[\"labels\"] = labels\n","\n","        print(labels)\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btBDdQxZenKB"},"outputs":[],"source":["data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"]},{"cell_type":"markdown","metadata":{"id":"9T8bZkoyLbvg"},"source":["### Métrique d'évaluation"]},{"cell_type":"markdown","metadata":{"id":"26kA0XtNLgW0"},"source":["Courament, le Word Error Rate (WER) est utilisé mais dans notre cas (pour des plaques d'immatriculation) il est plus pertinent d'utiliser le Character Error Rate (CER). On peut aussi utiliser le Phoneme Error Rate (PER) qui est néanmoins beaucoup moins courant"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nK5Nc-6jepLh","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["0305012b69f240d4a010ffa64a3029ad","0b76ff93f9304f0c8203ad67d4b6c210","4c933c2853ea49eba2ad4a89c1c0d1c0","512b9bf75fd6445bbeb469c21c040b08","7bdf8cc894d741919659d23344aeac67","37c2623170ee41969efddd7f6c82280e","1bdc9e20f24c4f2899dad861e0da39d8","52f63f354f494aec9ffe7ba83e15ba50","e35cfdf75f4f4578b8360ebcf2b30afb","85d21e05a0c64eb4a4bc2bb1d7b22999","6432a44642234c7cb60e615585b4c500"]},"executionInfo":{"status":"ok","timestamp":1653927431043,"user_tz":-120,"elapsed":1522,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"}},"outputId":"3358abe6-59f8-4a58-b9d8-ee39fdb6b035"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0305012b69f240d4a010ffa64a3029ad"}},"metadata":{}}],"source":["from datasets import load_metric\n","cer_metric = load_metric(\"cer\", revision = \"master\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk1chH45esAO"},"outputs":[],"source":["import numpy as np\n","def compute_metrics(pred):\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n","\n","    pred_str = processor.batch_decode(pred_ids)\n","    # we do not want to group tokens when computing the metrics\n","    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n","\n","    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"cer\": cer}"]},{"cell_type":"markdown","metadata":{"id":"DkC1Te9KL8Js"},"source":["### Importation du modèle pré-entrainé"]},{"cell_type":"markdown","metadata":{"id":"A7bgSoxx8jwW"},"source":["Maintenant, nous pouvons charger le modèle Wav2Vec2 pré-entraîné en français. Le `pad_token_id` du tokenizer doit être défini comme le `pad_token_id` du modèle ou, dans le cas de Wav2Vec2ForCTC, comme le token vide de CTC.\n","\n","Pour économiser la mémoire du GPU, nous activons le point de contrôle du gradient de PyTorch et définissons également la réduction des pertes à \"moyenne\".\n","\n","On peut trouver [ici](https://huggingface.co/models?sort=downloads&search=wav2vec) plusieurs modèles pré-entrainé de Wav2vec2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZX8w2dVB8oA"},"outputs":[],"source":["model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-french\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["fc21186b89054c37ac025d7bc0e07b88","09ce4655b82146da9e62077baaec01d6","f8818d52b4214256b9afe75cb8f92f95","2bb3d63c87124a04aca996b22561fe16","53042dff697845e99c092cb5845c2f74","9c06f28515c848218e71550b42c217ff","43ee2c0059a244ed85bf1c54cf67156a","ee2d70bbc0084ff5932db403e71108a6","eff7660d81734d06b75e915b222cfdba","300d2f3c86e848eb9a7a0186309194da","b0351887dfd442f5a5ec6647bb73d0c9","f89adcda18f2422fa33897e511a9b670","7aaf1a67c94a46e79da1a4b8d689137b","ef877f9caf2f441baa7eafda184d993e","4068b21ec91241ea891c9cf1cbf2db73","7d373366208748cda67f9e5b77319303","7c85039f5aa348d3a1db86ba41e68327","69c979d0225b47e1a45019783b95658d","779bc26c39c34eb5bc43a1c5d2554219","b7173b37f86b43e885d97366b9478b95","b3ae374aa80249b3b133f94dd59eb115","1b0d5f1a40634de5bcbcedc18cc6930f"]},"id":"PtKVzM_7iJ3a","executionInfo":{"status":"ok","timestamp":1653927459524,"user_tz":-120,"elapsed":28491,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"}},"outputId":"857eb1c6-7fdf-40ef-a6c5-4d6efcaf3fb4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc21186b89054c37ac025d7bc0e07b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f89adcda18f2422fa33897e511a9b670"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'\\nattention_dropout=0.1,\\n    hidden_dropout=0.1,\\n    feat_proj_dropout=0.0,\\n    mask_time_prob=0.05,\\n    layerdrop=0.1,\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["from transformers import Wav2Vec2ForCTC\n","\n","model = Wav2Vec2ForCTC.from_pretrained(\n","    model_name , \n","    ctc_loss_reduction=\"mean\", \n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=59,\n",")\n","\n","\n","'''\n","attention_dropout=0.1,\n","    hidden_dropout=0.1,\n","    feat_proj_dropout=0.0,\n","    mask_time_prob=0.05,\n","    layerdrop=0.1,\n","'''"]},{"cell_type":"markdown","metadata":{"id":"-1lnQ1pLo8hc"},"source":["Le premier composant de Wav2vec2 est une pile de couches de réseaux de neurones convolutifs (CNN) qui sont utilisés pour extraire des caractéristiques accoustiquement signifcatives (mais contextuellement indépendantes) du signal audio brut. Cette partie du modèle a été suffisamment entrainée durant le pré-entrainement and comme indiqué par les auteurs, ne nécessite pas d'être fine-tune. Donc nous pouvons définir le paramètre `required_grad` en `False` pour tous les paramètres de la partie d'extraction de caractéristiques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FS8jgX69iZ7C"},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"markdown","metadata":{"id":"JEFKpS0lPXSJ"},"source":["### Paramètres d'entrainement"]},{"cell_type":"markdown","metadata":{"id":"Lu1LOhn09CPq"},"source":["Pour la partie finale, on définit tous les paramètres relatifs à l'entrainement. Pour donner plus d'informations : \n","\n","`group_by_length` rend l'entrainement plus effiface en regroupant en un même lot (batch), les échantillons de la base d'entrainement ayant une même taille d'entrée. Cela peut accélérer significativement le temps d'entrainement en réduisant énormément le nombre de padding tokens qui passent par le modèle.\n","\n","`learning_rate` et `weight_decay` ont été heuristiquement régler jusqu'à que le fine-tuning soit stable. Notons que ces paramètres dépendent énormément de la base de données Timit et peut être non optimal pour d'autres base de données\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9ADwa7VPPFq"},"outputs":[],"source":["output_dir = \"/content/data/data_40/model/\" + model_name+\"_HL\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__4rgtOgiadU"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","  output_dir=output_dir,\n","  push_to_hub=False,\n","  group_by_length=True,\n","  per_device_train_batch_size=16,\n","  evaluation_strategy=\"epoch\",\n","  logging_strategy=\"epoch\",\n","  num_train_epochs=100,\n","  save_steps=500,\n","  eval_steps=500,\n","  logging_steps=500,\n","  learning_rate=1e-3,\n","  weight_decay=0.000,\n","  warmup_steps=1000,\n","  save_total_limit=2,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_BmEYUhic8G"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model.cuda(),\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=timit_prepared[\"train\"],\n","    eval_dataset=timit_prepared[\"test\"],\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"markdown","metadata":{"id":"sBzPO1_eijsX"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrFmKy-yi2Se","colab":{"base_uri":"https://localhost:8080/","height":433},"outputId":"bf3da52d-996b-479c-fd6e-87cb3f38347b","executionInfo":{"status":"error","timestamp":1653927505350,"user_tz":-120,"elapsed":30042,"user":{"displayName":"Speach Brain","userId":"05709083519020097853"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 936\n","  Num Epochs = 50\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2950\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-82c64df3280a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         )\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_target_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mabove\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: pad() got an unexpected keyword argument 'device'"]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWKpm7hiuUDT"},"outputs":[],"source":["#trainer.push_to_hub() #Enregistrement sur huggingface"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tie1WQnwQaqe"},"outputs":[],"source":["#Enregistrement du processeur et modèle dans le répertoire\n","processor.save_pretrained(output_dir)\n","trainer.save_model(output_dir)"]},{"cell_type":"markdown","metadata":{"id":"eaezMd8CDQsL"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqYmdDNCDQJa"},"outputs":[],"source":["from transformers import AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2ForCTC\n","# Le processeur est une classe contenant le feature extractor ainsi que le tokenizer\n","processor = Wav2Vec2Processor.from_pretrained(model_name)\n","model = Wav2Vec2ForCTC.from_pretrained(model_name).cuda()\n","\n","\"\"\"\n","model_name = \"Ilyes/wav2vec2-xls-fr\"\n","model = Wav2Vec2ForCTC.from_pretrained(model_name) ## Modèle brut sans fine \n","processor = Wav2Vec2Processor.from_pretrained(model_name)  \n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cNMP6P4Djch"},"outputs":[],"source":["def map_to_result(batch):\n","  input_values = processor(\n","      batch[\"speech\"], \n","      sampling_rate=batch[\"sampling_rate\"], \n","      return_tensors=\"pt\", device=\"cuda\"\n","  ).input_values\n","\n","  with torch.no_grad():\n","    logits = model(input_values).logits\n","\n","  pred_ids = torch.argmax(logits, dim=-1)\n","  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n","  \n","  return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Fl9ZPBkDk9P"},"outputs":[],"source":["import torch\n","results_train = timit[\"train\"].map(map_to_result)\n","results_test = timit[\"test\"].map(map_to_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCt0Z0MMEFJX"},"outputs":[],"source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=1):\n","    assert num_examples <= len(dataset), \"On vérifie que le nombre d'exemples à afficher et inférieur au nombres de données\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g9V_5XgeJGk"},"outputs":[],"source":["print(\"Test CER: {:.3f}\".format(cer_metric.compute(predictions=results_train[\"pred_str\"], references=results_train[\"target_text\"]))) # AFfiche score CER\n","\n","df = pd.DataFrame(results_train.remove_columns([\"speech\",\"sampling_rate\"]))\n","display(HTML(df.to_html())) # Affiche prédictions dans l'ordre"]},{"cell_type":"code","source":["print(\"Test CER: {:.3f}\".format(cer_metric.compute(predictions=results_test[\"pred_str\"], references=results_test[\"target_text\"]))) # AFfiche score CER\n","\n","df = pd.DataFrame(results_test.remove_columns([\"speech\",\"sampling_rate\"]))\n","display(HTML(df.to_html())) # Affiche prédictions dans l'ordre"],"metadata":{"id":"TUr17Piie17L"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Fine_tuned_training_and_evaluation.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0305012b69f240d4a010ffa64a3029ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b76ff93f9304f0c8203ad67d4b6c210","IPY_MODEL_4c933c2853ea49eba2ad4a89c1c0d1c0","IPY_MODEL_512b9bf75fd6445bbeb469c21c040b08"],"layout":"IPY_MODEL_7bdf8cc894d741919659d23344aeac67"}},"0b76ff93f9304f0c8203ad67d4b6c210":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c2623170ee41969efddd7f6c82280e","placeholder":"​","style":"IPY_MODEL_1bdc9e20f24c4f2899dad861e0da39d8","value":"Downloading builder script: "}},"4c933c2853ea49eba2ad4a89c1c0d1c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f63f354f494aec9ffe7ba83e15ba50","max":2160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e35cfdf75f4f4578b8360ebcf2b30afb","value":2160}},"512b9bf75fd6445bbeb469c21c040b08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85d21e05a0c64eb4a4bc2bb1d7b22999","placeholder":"​","style":"IPY_MODEL_6432a44642234c7cb60e615585b4c500","value":" 5.59k/? [00:00&lt;00:00, 227kB/s]"}},"7bdf8cc894d741919659d23344aeac67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37c2623170ee41969efddd7f6c82280e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bdc9e20f24c4f2899dad861e0da39d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52f63f354f494aec9ffe7ba83e15ba50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e35cfdf75f4f4578b8360ebcf2b30afb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85d21e05a0c64eb4a4bc2bb1d7b22999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6432a44642234c7cb60e615585b4c500":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc21186b89054c37ac025d7bc0e07b88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09ce4655b82146da9e62077baaec01d6","IPY_MODEL_f8818d52b4214256b9afe75cb8f92f95","IPY_MODEL_2bb3d63c87124a04aca996b22561fe16"],"layout":"IPY_MODEL_53042dff697845e99c092cb5845c2f74"}},"09ce4655b82146da9e62077baaec01d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c06f28515c848218e71550b42c217ff","placeholder":"​","style":"IPY_MODEL_43ee2c0059a244ed85bf1c54cf67156a","value":"Downloading: 100%"}},"f8818d52b4214256b9afe75cb8f92f95":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee2d70bbc0084ff5932db403e71108a6","max":1531,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eff7660d81734d06b75e915b222cfdba","value":1531}},"2bb3d63c87124a04aca996b22561fe16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_300d2f3c86e848eb9a7a0186309194da","placeholder":"​","style":"IPY_MODEL_b0351887dfd442f5a5ec6647bb73d0c9","value":" 1.50k/1.50k [00:00&lt;00:00, 61.5kB/s]"}},"53042dff697845e99c092cb5845c2f74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c06f28515c848218e71550b42c217ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43ee2c0059a244ed85bf1c54cf67156a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2d70bbc0084ff5932db403e71108a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff7660d81734d06b75e915b222cfdba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"300d2f3c86e848eb9a7a0186309194da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0351887dfd442f5a5ec6647bb73d0c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f89adcda18f2422fa33897e511a9b670":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7aaf1a67c94a46e79da1a4b8d689137b","IPY_MODEL_ef877f9caf2f441baa7eafda184d993e","IPY_MODEL_4068b21ec91241ea891c9cf1cbf2db73"],"layout":"IPY_MODEL_7d373366208748cda67f9e5b77319303"}},"7aaf1a67c94a46e79da1a4b8d689137b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c85039f5aa348d3a1db86ba41e68327","placeholder":"​","style":"IPY_MODEL_69c979d0225b47e1a45019783b95658d","value":"Downloading: 100%"}},"ef877f9caf2f441baa7eafda184d993e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_779bc26c39c34eb5bc43a1c5d2554219","max":1262175703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7173b37f86b43e885d97366b9478b95","value":1262175703}},"4068b21ec91241ea891c9cf1cbf2db73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3ae374aa80249b3b133f94dd59eb115","placeholder":"​","style":"IPY_MODEL_1b0d5f1a40634de5bcbcedc18cc6930f","value":" 1.18G/1.18G [00:22&lt;00:00, 60.9MB/s]"}},"7d373366208748cda67f9e5b77319303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c85039f5aa348d3a1db86ba41e68327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69c979d0225b47e1a45019783b95658d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"779bc26c39c34eb5bc43a1c5d2554219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7173b37f86b43e885d97366b9478b95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3ae374aa80249b3b133f94dd59eb115":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b0d5f1a40634de5bcbcedc18cc6930f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}